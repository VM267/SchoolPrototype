Supportive Check-In Signal Prototype
This repository contains an interactive prototype designed to help school staff, reviewers, and policymakers explore how AI can support early, human-led student check-ins in a responsible, transparent, and ethical way.
Rather than predicting outcomes or automating decisions, this project demonstrates how carefully constrained AI systems can surface attention while preserving human judgment, fairness, and interpretability.
Core contribution
The primary contribution of this project is not a predictive model, but a design philosophy.
This system intentionally limits what AI is allowed to do in order to:
avoid harm from biased or opaque predictions,


preserve human decision-making authority,


and remain explainable, auditable, and ethically defensible.


It is designed as a model for responsible educational AI, not an early-warning or surveillance system.
What this system is
The system generates a Support Signal, a number from 0 to 100, indicating when a student’s recent patterns may warrant a human-led supportive check-in.
The signal is meant to surface attention, not to decide action.
What this system is not
This system does not:
predict student outcomes,


rank or label students,


automate decisions,


trigger discipline,


replace educators, counselors, or human judgment.


Using the system in any of these ways would violate its design intent.
How it works at a high level
Student data (synthetic only)
The system analyzes synthetic student records that include metrics such as grades, absences, tardies, discipline events, and truancy days.


Recency-weighted signals
More recent data is weighted more heavily than older data, reflecting how support needs change over time.


Contextual interpretation using schools
A selected school provides contextual benchmarks, not comparisons or labels.
 Choosing a school changes how patterns are interpreted, not the student’s data itself.


Human-controlled thresholds and weights
Humans determine:


how conservative the system is,


which metrics matter more,


and when a check-in is suggested.


Explainable outputs
The system shows which indicators contributed most to the signal so reviewers can clearly understand why attention is being surfaced.


Interpreting the Support Signal
A higher Support Signal means there is more reason for human attention, not higher risk or severity.


A lower Support Signal does not mean a student is “fine.” It means the system does not currently see a strong reason to prompt review.


The signal should always be interpreted alongside:


human judgment,


local knowledge,


and follow-up conversations.


The signal supports humans. It does not override them.
Thresholds and weights
Threshold
Default threshold: 75


Above threshold: support check-in suggested


Below threshold: no check-in suggested


The threshold exists to help staff manage attention and workload.
There is no universally “correct” value. It reflects staff capacity, philosophy, and context.
Weights
Weights control the relative importance of each metric.


Weights are human-set and must sum to 1 to preserve interpretability.


Weights might be adjusted when:
attendance becomes a priority,


grading practices change,


certain metrics become unreliable,


staff want earlier or later signals.


Ethics and safeguards
Ethics are enforced at the system level, not assumed.
Protected attributes such as race, gender, income, disability status, and known proxies are explicitly rejected at ingestion.


The system does not attempt bias correction using protected attributes.


Indicators throughout the app show:


what data was used,


what data was blocked,


when protected attributes were detected and removed.


These safeguards exist so reviewers can verify ethical constraints are enforced, not merely claimed.
Evaluation philosophy
Because the system is intentionally non-predictive, success is not measured by predictive accuracy.
Instead, evaluation focuses on:
logical consistency,


interpretability,


transparency,


and alignment with ethical constraints.


Synthetic student profiles with distinct patterns are used to verify that:
similar patterns produce consistent signals,


protected attributes never influence outputs,


and explanations remain clear and stable.


Getting started
Run the app locally
pip install -r requirements.txt
streamlit run app/Home.py

When the app opens, start on the Home page and read the disclaimers. They explain system limits and proper interpretation.
All student data included in this prototype is synthetic.
The system is intentionally designed so it could operate without real student data.
How this system should be used
This prototype works best as:
a prioritization aid,


a conversation starter,


a transparency demonstration,


a support-planning tool.


It should always be paired with human judgment and follow-up.
Why this prototype exists
This project is intentionally limited.
Those limits make the system:
explainable,


auditable,


defensible,


ethically aligned,


and trustworthy to the people it affects.


The goal is not to show how powerful AI can be, but how carefully it can be applied.
If AI is used in schools, it should look more like this.
